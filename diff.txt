diff --git a/llamafile/server/client.h b/llamafile/server/client.h
index 74d1314e6..f82eed422 100644
--- a/llamafile/server/client.h
+++ b/llamafile/server/client.h
@@ -25,6 +25,7 @@
 #include <optional>
 #include <string>
 #include <sys/resource.h>
+#include "llama.cpp/common.h"
 
 #define HasHeader(H) (!!msg_.headers[H].a)
 #define HeaderData(H) (ibuf_.p + msg_.headers[H].a)
@@ -141,13 +142,4 @@ struct Client
 } // namespace lf
 
 // Global LoRA adapter storage - extern declarations (outside namespace to match definitions in prog.cpp)
-#define MAX_LORA_ADAPTERS 8
-struct lora_adapter_container {
-    struct llama_lora_adapter* adapter;
-    float scale;
-    std::string name;  // Model/adapter name for identification
-    bool applied;      // Whether this adapter is currently applied to slots
-};
-
-extern struct lora_adapter_container g_lora_adapters[MAX_LORA_ADAPTERS];
-extern int g_lora_adapters_count;
+// Remove the custom lora_adapter_container - we'll use llama.cpp's structure instead
diff --git a/llamafile/server/lora_adapters.cpp b/llamafile/server/lora_adapters.cpp
index 35e55198d..c048a2a4e 100644
--- a/llamafile/server/lora_adapters.cpp
+++ b/llamafile/server/lora_adapters.cpp
@@ -17,6 +17,7 @@
 
 #include "client.h"
 #include "llama.cpp/llama.h"
+#include "llama.cpp/common.h"
 #include "llamafile/json.h"
 #include "llamafile/llamafile.h"
 #include "llamafile/server/log.h"
@@ -29,9 +30,7 @@
 using jt::Json;
 
 // External declarations for global LoRA adapter storage from prog.cpp (outside namespace)
-// Note: struct lora_adapter_container and MAX_LORA_ADAPTERS are already defined in client.h
-extern struct lora_adapter_container g_lora_adapters[MAX_LORA_ADAPTERS];
-extern int g_lora_adapters_count;
+extern std::vector<llama_lora_adapter_container> g_lora_adapters;
 
 namespace lf {
 namespace server {
@@ -46,12 +45,12 @@ Client::lora_adapters()
         json.setArray();
         std::vector<Json>& json_array = json.getArray();
         
-        for (int i = 0; i < g_lora_adapters_count; i++) {
+        for (size_t i = 0; i < ::g_lora_adapters.size(); i++) {
             Json adapter;
             adapter.setObject();
-            adapter["id"] = i;
-            adapter["path"] = g_lora_adapters[i].name;  // Use name as path for now
-            adapter["scale"] = g_lora_adapters[i].scale;
+            adapter["id"] = (int)i;
+            adapter["path"] = ::g_lora_adapters[i].path;
+            adapter["scale"] = ::g_lora_adapters[i].scale;
             json_array.push_back(adapter);
         }
         
@@ -93,7 +92,7 @@ bool
 Client::handle_apply_adapters(Json& json)
 {
     // Get active slots and apply current adapters to them
-    if (g_lora_adapters_count == 0) {
+    if (::g_lora_adapters.empty()) {
         Json response;
         response["success"] = false;
         response["message"] = "No adapters loaded to apply";
@@ -103,14 +102,34 @@ Client::handle_apply_adapters(Json& json)
         return send_response(obuf_.p, p, response.toString());
     }
     
-    // Apply adapters to all slots via the server
-    // Note: This would require coordination with the slot management system
-    SLOG("applying %d LoRA adapter(s) to all active slots", g_lora_adapters_count);
+    // Apply adapters to all slots via the server using llama.cpp unified function
+    SLOG("applying %d LoRA adapter(s) to all active slots using llama.cpp unified function", 
+         (int)::g_lora_adapters.size());
+    
+    // Apply to all active slots
+    Slots* slots = worker_->server_->slots_;
+    pthread_mutex_lock(&slots->lock_);
+    
+    for (size_t i = 0; i < slots->slots_.size(); ++i) {
+        Slot* slot = slots->slots_[i].get();
+        if (slot->ctx_) {
+            SLOG("applying LoRA adapters to slot #%d", slot->id_);
+            llama_lora_adapters_apply(slot->ctx_, ::g_lora_adapters);
+            
+            // CRITICAL: Mark slot for refresh to handle LoRA changes properly
+            // The slot's prefill() mechanism will intelligently preserve system prompts
+            // and only re-evaluate what's necessary when the next request comes in
+            slot->mark_for_refresh();
+            SLOG("marked slot #%d for refresh after LoRA application", slot->id_);
+        }
+    }
+    
+    pthread_mutex_unlock(&slots->lock_);
     
     Json response;
     response["success"] = true;
     response["message"] = "Adapters applied to active slots";
-    response["adapters_applied"] = g_lora_adapters_count;
+    response["adapters_applied"] = (int)::g_lora_adapters.size();
     
     char* p = append_http_response_message(obuf_.p, 200);
     p = stpcpy(p, "Content-Type: application/json\r\n");
@@ -128,18 +147,6 @@ Client::handle_load_adapter(Json& json)
     std::string adapter_path = json["path"].getString();
     float scale = json.contains("scale") ? json["scale"].getNumber() : 1.0f;
     
-    // Check if we have room for more adapters
-    if (g_lora_adapters_count >= MAX_LORA_ADAPTERS) {
-        Json response;
-        response["success"] = false;
-        response["message"] = "Maximum number of adapters already loaded";
-        response["max_adapters"] = MAX_LORA_ADAPTERS;
-        
-        char* p = append_http_response_message(obuf_.p, 400);
-        p = stpcpy(p, "Content-Type: application/json\r\n");
-        return send_response(obuf_.p, p, response.toString());
-    }
-    
     // Check if file exists
     if (!std::filesystem::exists(adapter_path)) {
         Json response;
@@ -167,11 +174,15 @@ Client::handle_load_adapter(Json& json)
         return send_response(obuf_.p, p, response.toString());
     }
     
+    // Create the adapter container
+    llama_lora_adapter_container adapter_container;
+    adapter_container.path = adapter_path;
+    adapter_container.scale = scale;
+    adapter_container.adapter = adapter;
+    
     // Store the adapter
-    int index = g_lora_adapters_count;
-    g_lora_adapters[index].adapter = adapter;
-    g_lora_adapters[index].scale = scale;
-    g_lora_adapters_count++;
+    int index = (int)::g_lora_adapters.size();
+    ::g_lora_adapters.push_back(adapter_container);
     
     SLOG("successfully loaded LoRA adapter #%d from %s", index, adapter_path.c_str());
     
@@ -181,7 +192,7 @@ Client::handle_load_adapter(Json& json)
     response["index"] = index;
     response["path"] = adapter_path;
     response["scale"] = scale;
-    response["total_adapters"] = g_lora_adapters_count;
+    response["total_adapters"] = (int)::g_lora_adapters.size();
     
     char* p = append_http_response_message(obuf_.p, 200);
     p = stpcpy(p, "Content-Type: application/json\r\n");
@@ -192,18 +203,16 @@ bool
 Client::handle_clear_adapters()
 {
     // Clear all loaded adapters
-    SLOG("clearing all %d LoRA adapter(s)", g_lora_adapters_count);
+    SLOG("clearing all %d LoRA adapter(s)", (int)::g_lora_adapters.size());
     
-    for (int i = 0; i < g_lora_adapters_count; i++) {
-        if (g_lora_adapters[i].adapter) {
-            llama_lora_adapter_free(g_lora_adapters[i].adapter);
-            g_lora_adapters[i].adapter = nullptr;
-            g_lora_adapters[i].scale = 0.0f;
+    int cleared_count = (int)::g_lora_adapters.size();
+    for (auto& la : ::g_lora_adapters) {
+        if (la.adapter) {
+            llama_lora_adapter_free(la.adapter);
         }
     }
     
-    int cleared_count = g_lora_adapters_count;
-    g_lora_adapters_count = 0;
+    ::g_lora_adapters.clear();
     
     SLOG("cleared %d LoRA adapter(s)", cleared_count);
     
@@ -225,11 +234,6 @@ Client::handle_upstream_lora_apply(Json& json)
     std::vector<Json>& json_array = json.getArray();
     SLOG("applying LoRA configuration with %d entries", (int)json_array.size());
     
-    // First, reset all adapter scales to 0.0 (disabled)
-    for (int i = 0; i < g_lora_adapters_count; i++) {
-        g_lora_adapters[i].applied = false;
-    }
-    
     // Process each entry in the array
     for (size_t i = 0; i < json_array.size(); i++) {
         Json& entry = json_array[i];
@@ -246,22 +250,21 @@ Client::handle_upstream_lora_apply(Json& json)
         float scale = entry["scale"].getNumber();
         
         // Validate ID range
-        if (id < 0 || id >= g_lora_adapters_count) {
+        if (id < 0 || id >= (int)::g_lora_adapters.size()) {
             return send_error(400, "Invalid adapter ID");
         }
         
         // Update the adapter configuration
-        g_lora_adapters[id].scale = scale;
-        g_lora_adapters[id].applied = (scale > 0.0f);
+        ::g_lora_adapters[id].scale = scale;
         
         char scale_buf[32];
         snprintf(scale_buf, sizeof(scale_buf), "%.2f", scale);
         SLOG("set LoRA adapter %d ('%s') scale to %s", 
-             id, g_lora_adapters[id].name.c_str(), scale_buf);
+             id, ::g_lora_adapters[id].path.c_str(), scale_buf);
     }
     
-    // Re-apply LoRA adapters to all active slots with updated scales
-    SLOG("re-applying LoRA adapters to all active slots");
+    // Re-apply LoRA adapters to all active slots with updated scales using llama.cpp unified function
+    SLOG("re-applying LoRA adapters to all active slots using llama.cpp unified function");
     Slots* slots = worker_->server_->slots_;
     
     // Lock the slots to prevent concurrent access during LoRA re-application
@@ -271,32 +274,13 @@ Client::handle_upstream_lora_apply(Json& json)
         Slot* slot = slots->slots_[i].get();
         if (slot->ctx_) {
             SLOG("re-applying LoRA adapters to slot #%d", slot->id_);
+            llama_lora_adapters_apply(slot->ctx_, ::g_lora_adapters);
             
-            // Clear existing LoRA adapters from this context
-            llama_lora_adapter_clear(slot->ctx_);
-            
-            // Use the same approach as slot initialization: get all adapters via the function
-            struct llama_lora_adapter* adapters[MAX_LORA_ADAPTERS];
-            float scales[MAX_LORA_ADAPTERS];
-            int adapter_count = llamafiler_get_lora_adapters(adapters, scales, MAX_LORA_ADAPTERS);
-            
-            SLOG("got %d LoRA adapters from llamafiler_get_lora_adapters for slot #%d", adapter_count, slot->id_);
-            
-            // Re-apply all adapters with their current scales
-            for (int j = 0; j < adapter_count; ++j) {
-                char scale_buf[32];
-                snprintf(scale_buf, sizeof(scale_buf), "%.2f", scales[j]);
-                SLOG("processing LoRA adapter %d with scale %s", j, scale_buf);
-                if (scales[j] > 0.0f) {
-                    if (llama_lora_adapter_set(slot->ctx_, adapters[j], scales[j]) != 0) {
-                        SLOG("failed to re-apply LoRA adapter %d to slot #%d", j, slot->id_);
-                    } else {
-                        SLOG("re-applied LoRA adapter %d to slot #%d with scale %s", j, slot->id_, scale_buf);
-                    }
-                } else {
-                    SLOG("skipping LoRA adapter %d (scale %s <= 0)", j, scale_buf);
-                }
-            }
+            // CRITICAL: Mark slot for refresh to handle LoRA changes properly
+            // The slot's prefill() mechanism will intelligently preserve system prompts
+            // and only re-evaluate what's necessary when the next request comes in
+            slot->mark_for_refresh();
+            SLOG("marked slot #%d for refresh after LoRA update", slot->id_);
         }
     }
     
@@ -307,12 +291,13 @@ Client::handle_upstream_lora_apply(Json& json)
     Json response;
     response.setArray();
     std::vector<Json>& response_array = response.getArray();
-    for (int i = 0; i < g_lora_adapters_count; i++) {
+    
+    for (size_t i = 0; i < ::g_lora_adapters.size(); i++) {
         Json adapter;
         adapter.setObject();
-        adapter["id"] = i;
-        adapter["path"] = g_lora_adapters[i].name;
-        adapter["scale"] = g_lora_adapters[i].scale;
+        adapter["id"] = (int)i;
+        adapter["path"] = ::g_lora_adapters[i].path;
+        adapter["scale"] = ::g_lora_adapters[i].scale;
         response_array.push_back(adapter);
     }
     
diff --git a/llamafile/server/prog.cpp b/llamafile/server/prog.cpp
index a21c80961..89ae3f12d 100644
--- a/llamafile/server/prog.cpp
+++ b/llamafile/server/prog.cpp
@@ -26,31 +26,21 @@
 #include "llamafile/server/tokenbucket.h"
 #include "llamafile/server/utils.h"
 #include "llamafile/version.h"
+#include "llama.cpp/common.h"
 #include <cassert>
 #include <cosmo.h>
 
-// Global LoRA adapter storage for multiple adapters
-#define MAX_LORA_ADAPTERS 8
-#include <string>
-struct lora_adapter_container {
-    struct llama_lora_adapter* adapter;
-    float scale;
-    std::string name;  // Model/adapter name for identification
-    bool applied;      // Whether this adapter is currently applied to slots
-};
-
-// Make these externally accessible for HTTP endpoint
-struct lora_adapter_container g_lora_adapters[MAX_LORA_ADAPTERS] = {};
-int g_lora_adapters_count = 0;
+// Global LoRA adapter storage using llama.cpp structures
+std::vector<llama_lora_adapter_container> g_lora_adapters;
 
 // Function to get the first global LoRA adapter for backward compatibility
 extern "C" struct llama_lora_adapter* llamafiler_get_lora_adapter() {
-    return g_lora_adapters_count > 0 ? g_lora_adapters[0].adapter : nullptr;
+    return g_lora_adapters.empty() ? nullptr : g_lora_adapters[0].adapter;
 }
 
 // Function to get all LoRA adapters and their count
 extern "C" int llamafiler_get_lora_adapters(struct llama_lora_adapter** adapters, float* scales, int max_adapters) {
-    int count = g_lora_adapters_count < max_adapters ? g_lora_adapters_count : max_adapters;
+    int count = std::min((int)g_lora_adapters.size(), max_adapters);
     for (int i = 0; i < count; i++) {
         adapters[i] = g_lora_adapters[i].adapter;
         scales[i] = g_lora_adapters[i].scale;
@@ -129,38 +119,31 @@ main(int argc, char* argv[])
             char scale_buf[32];
             snprintf(scale_buf, sizeof(scale_buf), "%.2f", FLAG_lora_adapters[i].scale);
             
-            // Generate model name from filename
+            // Generate model name from filename for identification
             const char* path = FLAG_lora_adapters[i].path;
             const char* filename = strrchr(path, '/');
             filename = filename ? filename + 1 : path;
             
-            // Remove file extension for cleaner name
-            std::string model_name(filename);
-            size_t dot_pos = model_name.find_last_of('.');
-            if (dot_pos != std::string::npos) {
-                model_name = model_name.substr(0, dot_pos);
-            }
-            
             SLOG("loading LoRA adapter %d ('%s') from %s with scale %s", i + 1, 
-                 model_name.c_str(), path, scale_buf);
-                 
-            g_lora_adapters[i].adapter = llama_lora_adapter_init(model, path);
-            g_lora_adapters[i].scale = FLAG_lora_adapters[i].scale;
-            g_lora_adapters[i].name = model_name;
-            g_lora_adapters[i].applied = !FLAG_lora_init_without_apply;  // Apply unless flag is set
+                 filename, path, scale_buf);
+            
+            llama_lora_adapter_container adapter_container;
+            adapter_container.path = std::string(path);
+            adapter_container.scale = FLAG_lora_adapters[i].scale;
+            adapter_container.adapter = llama_lora_adapter_init(model, path);
             
-            if (!g_lora_adapters[i].adapter) {
+            if (!adapter_container.adapter) {
                 fprintf(stderr, "%s: failed to load LoRA adapter from %s\n", FLAG_model, path);
                 // Cleanup previously loaded adapters
-                for (int j = 0; j < i; j++) {
-                    if (g_lora_adapters[j].adapter) {
-                        llama_lora_adapter_free(g_lora_adapters[j].adapter);
+                for (auto& la : g_lora_adapters) {
+                    if (la.adapter) {
+                        llama_lora_adapter_free(la.adapter);
                     }
                 }
                 llama_free_model(model);
                 exit(1);
             }
-            g_lora_adapters_count++;
+            g_lora_adapters.push_back(adapter_container);
         }
         
         if (FLAG_lora_init_without_apply) {
@@ -203,9 +186,9 @@ main(int argc, char* argv[])
     delete slots;
     
     // Cleanup LoRA adapters
-    for (int i = 0; i < g_lora_adapters_count; i++) {
-        if (g_lora_adapters[i].adapter) {
-            llama_lora_adapter_free(g_lora_adapters[i].adapter);
+    for (auto& la : g_lora_adapters) {
+        if (la.adapter) {
+            llama_lora_adapter_free(la.adapter);
         }
     }
     
diff --git a/llamafile/server/slot.cpp b/llamafile/server/slot.cpp
index 55138417b..a081d69df 100644
--- a/llamafile/server/slot.cpp
+++ b/llamafile/server/slot.cpp
@@ -18,6 +18,7 @@
 #include "slot.h"
 #include "llama.cpp/llava/clip.h"
 #include "llama.cpp/llava/llava.h"
+#include "llama.cpp/common.h"
 #include "llamafile/image.h"
 #include "llamafile/llama.h"
 #include "llamafile/llamafile.h"
@@ -32,6 +33,9 @@
 #include <cassert>
 #include <cosmo.h>
 
+// External declaration for global LoRA adapter storage
+extern std::vector<llama_lora_adapter_container> g_lora_adapters;
+
 namespace lf {
 namespace server {
 
@@ -79,7 +83,7 @@ Slot::describe_error(int err)
     }
 }
 
-Slot::Slot(int id, llama_model* model) : id_(id), model_(model)
+Slot::Slot(int id, llama_model* model) : id_(id), model_(model), needs_refresh_(false)
 {
     dll_init(&elem_);
     last_used_ = time(0);
@@ -126,24 +130,16 @@ Slot::start()
     if (!(ctx_ = llama_new_context_with_model(model_, cparams)))
         return false;
     
-    // Apply LoRA adapters if available
-    struct llama_lora_adapter* adapters[MAX_LORA_ADAPTERS];
-    float scales[MAX_LORA_ADAPTERS];
-    int adapter_count = llamafiler_get_lora_adapters(adapters, scales, MAX_LORA_ADAPTERS);
-    
-    if (adapter_count > 0) {
-        SLOG("applying %d LoRA adapter(s) to slot #%d", adapter_count, id_);
-        for (int i = 0; i < adapter_count; i++) {
-            if (llama_lora_adapter_set(ctx_, adapters[i], scales[i]) != 0) {
-                SLOG("failed to apply LoRA adapter %d to slot #%d", i + 1, id_);
-                llama_free(ctx_);
-                ctx_ = nullptr;
-                return false;
-            }
-            char scale_buf[32];
-            snprintf(scale_buf, sizeof(scale_buf), "%.2f", scales[i]);
-            SLOG("applied LoRA adapter %d to slot #%d with scale %s", i + 1, id_, scale_buf);
-        }
+    // Apply LoRA adapters if available using llama.cpp's unified function
+    if (!::g_lora_adapters.empty() && !FLAG_lora_init_without_apply) {
+        SLOG("applying %d LoRA adapter(s) to slot #%d using llama.cpp unified function", 
+             (int)::g_lora_adapters.size(), id_);
+        llama_lora_adapters_apply(ctx_, ::g_lora_adapters);
+    } else if (!::g_lora_adapters.empty() && FLAG_lora_init_without_apply) {
+        // When --lora-init-without-apply is set, explicitly clear any LoRA state
+        // to ensure no residual LoRA effects from model initialization
+        SLOG("clearing LoRA state for slot #%d (--lora-init-without-apply mode)", id_);
+        llama_lora_adapter_clear(ctx_);
     }
     
     if (FLAG_mmproj)
@@ -314,6 +310,15 @@ Slot::prefill(const std::vector<Atom>& atoms, const ProgressCallback& progress)
     if (!ctx_)
         return uninitialized;
 
+    // Check if we need to refresh due to LoRA adapter changes
+    if (needs_refresh_) {
+        SLOG("Refreshing slot due to LoRA adapter changes");
+        llama_kv_cache_clear(ctx_);
+        history_.clear();
+        needs_refresh_ = false;
+        // Fall through to normal prefill logic with cleared state
+    }
+
     // handle special case of empty prefill
     if (atoms.empty()) {
         llama_kv_cache_clear(ctx_);
@@ -458,5 +463,11 @@ Slot::dump(std::string* result)
     }
 }
 
+void
+Slot::mark_for_refresh()
+{
+    needs_refresh_ = true;
+}
+
 } // namespace server
 } // namespace lf
diff --git a/llamafile/server/slot.h b/llamafile/server/slot.h
index e8816c900..104aa7623 100644
--- a/llamafile/server/slot.h
+++ b/llamafile/server/slot.h
@@ -23,7 +23,6 @@
 #include <vector>
 
 #define SLOT(e) DLL_CONTAINER(Slot, elem_, e)
-#define MAX_LORA_ADAPTERS 8
 
 struct llama_context;
 struct llama_model;
@@ -66,6 +65,7 @@ struct Slot
     llama_context* ctx_ = nullptr;
     std::vector<Atom> history_;
     std::string system_fingerprint_;
+    bool needs_refresh_ = false;
 
     ~Slot();
     Slot(int, llama_model*);
@@ -79,6 +79,7 @@ struct Slot
     int prefill(const std::vector<Atom>&, const ProgressCallback& = nullptr);
     void tokenize(std::vector<Atom>*, std::string_view, bool);
     void dump(std::string*);
+    void mark_for_refresh();
 };
 
 } // namespace server
